import json
import math

import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import streamlit as st
from langchain_core.output_parsers import JsonOutputParser, StrOutputParser
from langchain_core.prompts import PromptTemplate

from modules.keyword_mining import KeywordMining
from modules.semantic_expander import SemanticExpander
from modules.topic_cluster import TopicCluster


INVALID_FS_CHARS = r'<>:"/\\|?*\n\r\t'


def sanitize_filename(name: str, max_len: int = 80) -> str:
    """Copy of utility from geo_tool, kept local to avoid circular imports."""
    if not name:
        return "untitled"
    name = name.strip()
    # å»¶ç»­ä¸»åº”ç”¨ä¸­çš„å‘½åæ¸…ç†è§„åˆ™
    import re  # å±€éƒ¨å¯¼å…¥ï¼Œé¿å…åœ¨æ¨¡å—é¡¶éƒ¨é‡å¤å¯¼å…¥

    name = re.sub(rf"[{re.escape(INVALID_FS_CHARS)}]", "_", name)
    name = re.sub(r"_+", "_", name).strip("_")
    return name[:max_len] if len(name) > max_len else name


def extract_json_array(text: str):
    """ä»æ¨¡å‹è¾“å‡ºä¸­æŠ½å– JSON æ•°ç»„ï¼ˆJsonOutputParser å¤±è´¥æ—¶å…œåº•ï¼‰ã€‚"""
    if not text:
        return None
    import re

    m = re.search(r"\[[\s\S]*\]", text)
    if not m:
        return None
    try:
        return json.loads(m.group(0))
    except Exception:
        return None


def render_tab_keywords(storage, ss_init, gen_llm, brand: str, advantages: str) -> None:
    """
    æ¸²æŸ“ Tab1ï¼šå…³é”®è¯è’¸é¦ã€‚

    è¯¥å®ç°ç›´æ¥ä» `geo_tool.py` ä¸­è¿ç§»è€Œæ¥ï¼Œä»…è¿›è¡Œäº†æœ€å°å¿…è¦çš„ç»“æ„è°ƒæ•´ï¼š
    - åŒ…è£…ä¸ºå‡½æ•°ï¼Œä¾¿äºä»ä¸»å…¥å£è°ƒç”¨
    - é€šè¿‡å‚æ•°æ¥æ”¶ `storage` / `ss_init` / `gen_llm` / `brand` / `advantages`
    """
    # ========== åŒºåŸŸ 1ï¼šæ¨¡å¼é€‰æ‹© ==========
    st.markdown("**ğŸ¯ ç”Ÿæˆæ¨¡å¼**")
    generation_mode = st.radio(
        "é€‰æ‹©ç”Ÿæˆæ¨¡å¼",
        ["AIç”Ÿæˆ", "æ‰˜è¯å·¥å…·", "æ··åˆæ¨¡å¼"],
        index=["AIç”Ÿæˆ", "æ‰˜è¯å·¥å…·", "æ··åˆæ¨¡å¼"].index(
            st.session_state.kw_generation_mode
        ),
        horizontal=True,
        key="kw_mode_radio",
        help="AIç”Ÿæˆï¼šä½¿ç”¨ LLM ç›´æ¥ç”Ÿæˆï¼›æ‰˜è¯å·¥å…·ï¼šåŸºäºè¯åº“ç»„åˆï¼›æ··åˆæ¨¡å¼ï¼šå…ˆç»„åˆå†æ¶¦è‰²",
    )
    st.session_state.kw_generation_mode = generation_mode
    st.markdown("---")

    # ========== åŒºåŸŸ 2ï¼šé…ç½®åŒºï¼ˆæ¡ä»¶æ˜¾ç¤ºï¼‰ ==========
    if generation_mode in ["æ‰˜è¯å·¥å…·", "æ··åˆæ¨¡å¼"]:
        # åˆå§‹åŒ–è¯åº“
        if st.session_state.wordbanks is None:
            st.session_state.wordbanks = st.session_state.keyword_tool.load_wordbanks()

        # åˆå§‹åŒ–ç»„åˆæ¨¡å¼é€‰æ‹©
        ss_init("selected_patterns", list(st.session_state.keyword_tool.combination_patterns))

        wordbanks = st.session_state.wordbanks

        # ç»„åˆæ¨¡å¼é€‰æ‹©
        with st.container(border=True):
            st.markdown("**ğŸ“ ç»„åˆæ¨¡å¼é€‰æ‹©**")
            pattern_descriptions = st.session_state.keyword_tool.get_pattern_descriptions()
            all_patterns = st.session_state.keyword_tool.combination_patterns

            # æ˜¾ç¤ºæ‰€æœ‰å¯ç”¨æ¨¡å¼
            pattern_options = []
            for pattern in all_patterns:
                pattern_str = "+".join(pattern)
                desc = pattern_descriptions.get(pattern_str, pattern_str)
                pattern_options.append((pattern_str, pattern, desc))

            # å¤šé€‰ç»„åˆæ¨¡å¼
            selected_pattern_strs = st.multiselect(
                "é€‰æ‹©è¦ä½¿ç”¨çš„ç»„åˆæ¨¡å¼ï¼ˆå¯å¤šé€‰ï¼‰",
                options=[opt[0] for opt in pattern_options],
                default=[
                    opt[0]
                    for opt in pattern_options
                    if opt[1] in st.session_state.selected_patterns
                ],
                key="kw_pattern_select",
                help="é€‰æ‹©è¦ä½¿ç”¨çš„ç»„åˆæ¨¡å¼ï¼Œè‡³å°‘é€‰æ‹©ä¸€ä¸ª",
            )

            # æ›´æ–°é€‰ä¸­çš„æ¨¡å¼
            selected_patterns = []
            for pattern_str, pattern, desc in pattern_options:
                if pattern_str in selected_pattern_strs:
                    selected_patterns.append(pattern)
            st.session_state.selected_patterns = (
                selected_patterns if selected_patterns else all_patterns
            )

            # æ˜¾ç¤ºæ¨¡å¼è¯´æ˜
            with st.expander("ğŸ“– ç»„åˆæ¨¡å¼è¯´æ˜", expanded=False):
                for pattern_str, pattern, desc in pattern_options:
                    st.markdown(f"**{pattern_str}**: {' + '.join(desc)}")

        # è¯åº“ç®¡ç†
        with st.container(border=True):
            st.markdown("**ğŸ“š è¯åº“ç®¡ç†**")
            wordbank_tab1, wordbank_tab2 = st.tabs(["ç¼–è¾‘è¯åº“", "å¯¼å…¥/å¯¼å‡º"])

            with wordbank_tab1:
                st.markdown("**è¯åº“ç¼–è¾‘**")
                bank_types = list(wordbanks.keys())

                # æ¨ªå‘å±•ç¤ºæ‰€æœ‰è¯åº“ç±»å‹ï¼ˆ6åˆ—ï¼‰
                st.caption(
                    "ğŸ’¡ æç¤ºï¼šæ‰€æœ‰è¯åº“ç±»å‹æ¨ªå‘å±•ç¤ºï¼Œå¯ç›´æ¥ç¼–è¾‘ï¼Œç‚¹å‡»å„åˆ—çš„ã€Œæ›´æ–°ã€æŒ‰é’®æˆ–ä½¿ç”¨ä¸‹æ–¹çš„ã€Œæ›´æ–°æ‰€æœ‰è¯åº“ã€æŒ‰é’®ä¿å­˜ä¿®æ”¹"
                )
                cols = st.columns(6)
                edited_wordbanks = {}

                for idx, bank_type in enumerate(bank_types):
                    with cols[idx]:
                        # æ˜¾ç¤ºè¯åº“ç±»å‹åç§°
                        st.markdown(f"**{bank_type}**")

                        # æ˜¾ç¤ºå½“å‰è¯åº“å†…å®¹
                        current_words = wordbanks.get(bank_type, [])
                        edited_words = st.text_area(
                            f"{bank_type} è¯æ±‡ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰",
                            "\n".join(current_words),
                            height=200,
                            key=f"kw_bank_edit_{bank_type}",
                            label_visibility="collapsed",
                        )

                        # ä¿å­˜ç¼–è¾‘å†…å®¹
                        edited_wordbanks[bank_type] = edited_words

                        # æ¯ä¸ªè¯åº“å•ç‹¬çš„æ›´æ–°æŒ‰é’®
                        if st.button(
                            "æ›´æ–°",
                            key=f"kw_update_{bank_type}",
                            use_container_width=True,
                        ):
                            new_words = [
                                w.strip() for w in edited_words.split("\n") if w.strip()
                            ]
                            wordbanks[bank_type] = new_words
                            st.session_state.wordbanks = wordbanks
                            st.success(f"âœ… {bank_type} å·²æ›´æ–°ï¼ˆ{len(new_words)} ä¸ªè¯æ±‡ï¼‰")
                            st.info(
                                "ğŸ’¡ æç¤ºï¼šè¯åº“å·²æ›´æ–°ï¼Œå»ºè®®é‡æ–°ç”Ÿæˆå…³é”®è¯ä»¥åº”ç”¨æ–°è¯åº“"
                            )
                            st.rerun()

                # ç»Ÿä¸€æ›´æ–°æ‰€æœ‰è¯åº“æŒ‰é’®
                st.markdown("---")
                if st.button(
                    "ğŸ’¾ æ›´æ–°æ‰€æœ‰è¯åº“",
                    use_container_width=True,
                    type="primary",
                    key="kw_update_all",
                ):
                    updated_count = 0
                    for bank_type, edited_text in edited_wordbanks.items():
                        new_words = [
                            w.strip() for w in edited_text.split("\n") if w.strip()
                        ]
                        if new_words != wordbanks.get(bank_type, []):
                            wordbanks[bank_type] = new_words
                            updated_count += 1

                    if updated_count > 0:
                        st.session_state.wordbanks = wordbanks
                        st.success(f"âœ… å·²æ›´æ–° {updated_count} ä¸ªè¯åº“")
                        st.info(
                            "ğŸ’¡ æç¤ºï¼šè¯åº“å·²æ›´æ–°ï¼Œå»ºè®®é‡æ–°ç”Ÿæˆå…³é”®è¯ä»¥åº”ç”¨æ–°è¯åº“"
                        )
                        st.rerun()
                    else:
                        st.info("æ²¡æœ‰è¯åº“éœ€è¦æ›´æ–°")

            with wordbank_tab2:
                st.markdown("**è¯åº“å¯¼å…¥/å¯¼å‡º**")
                # å¯¼å‡º
                wordbanks_json = json.dumps(wordbanks, ensure_ascii=False, indent=2)
                st.download_button(
                    "å¯¼å‡ºè¯åº“ï¼ˆJSONï¼‰",
                    wordbanks_json,
                    "wordbanks.json",
                    "application/json",
                    use_container_width=True,
                    key="kw_export_json",
                )

                st.markdown("---")

                # å¯¼å…¥
                uploaded_wordbanks = st.file_uploader(
                    "å¯¼å…¥è¯åº“ï¼ˆJSONï¼‰",
                    type=["json"],
                    key="kw_import_json",
                )
                if uploaded_wordbanks:
                    try:
                        imported = json.loads(
                            uploaded_wordbanks.read().decode("utf-8")
                        )
                        if isinstance(imported, dict):
                            st.session_state.wordbanks = imported
                            st.success("è¯åº“å¯¼å…¥æˆåŠŸï¼")
                            st.rerun()
                    except Exception as e:
                        st.error(f"å¯¼å…¥å¤±è´¥ï¼š{e}")

                st.markdown("---")

                # é‡ç½®ä¸ºé»˜è®¤è¯åº“
                if st.button(
                    "é‡ç½®ä¸ºé»˜è®¤è¯åº“",
                    use_container_width=True,
                    key="kw_reset_banks",
                ):
                    st.session_state.wordbanks = (
                        st.session_state.keyword_tool.load_wordbanks()
                    )
                    st.success("å·²é‡ç½®ä¸ºé»˜è®¤è¯åº“")
                    st.rerun()

        st.markdown("---")

    # ========== åŒºåŸŸ 3ï¼šç”Ÿæˆæ§åˆ¶ ==========
    with st.container(border=True):
        st.markdown("**âš™ï¸ ç”Ÿæˆæ§åˆ¶**")
        ss_init("kw_last_num", 20)  # ç¡®ä¿é»˜è®¤å€¼åˆå§‹åŒ–

        c1, c2, c3 = st.columns([2, 1, 1])
        with c1:
            st.session_state.kw_last_num = st.slider(
                "ç”Ÿæˆæ•°é‡",
                5,
                200,
                st.session_state.kw_last_num,
                key="kw_num",
                help="å»ºè®®èŒƒå›´ï¼š10-50 ä¸ªå…³é”®è¯",
            )
        with c2:
            # æ ¹æ®æ¨¡å¼è°ƒæ•´ç¦ç”¨æ¡ä»¶
            if generation_mode == "æ‰˜è¯å·¥å…·":
                run_kw_disabled = (
                    not st.session_state.get("selected_patterns")
                    or len(st.session_state.get("selected_patterns", [])) == 0
                )
            else:
                run_kw_disabled = (not st.session_state.cfg_valid) or (gen_llm is None)

            # é˜²æ­¢å¹¶å‘ç‚¹å‡»
            if st.session_state.get("kw_generating", False):
                run_kw_disabled = True

            run_kw = st.button(
                "ğŸš€ ç”Ÿæˆå…³é”®è¯",
                type="primary",
                use_container_width=True,
                disabled=run_kw_disabled,
                key="kw_run",
            )
        with c3:
            if st.button(
                "ğŸ—‘ï¸ æ¸…ç©ºç»“æœ", use_container_width=True, key="kw_clear"
            ):
                # æ¸…ç©ºæ‰€æœ‰ç›¸å…³çŠ¶æ€
                st.session_state.keywords = []
                st.session_state.expanded_keywords = []
                st.session_state.topic_clusters = []
                st.session_state.cluster_relationships = []
                st.session_state.cluster_stats = None
                st.session_state.content_planning = None
                st.session_state.mined_keywords = []
                st.session_state.competition_analysis = {}
                st.session_state.trend_analysis = {}
                st.session_state.value_matrix = {}
                st.session_state.keyword_recommendations = []
                st.toast("å·²æ¸…ç©ºæ‰€æœ‰å…³é”®è¯å’Œç›¸å…³æ•°æ®")
                st.rerun()

        if run_kw:
            # ========== å‚æ•°éªŒè¯ ==========
            if generation_mode == "AIç”Ÿæˆ":
                if not brand or not advantages:
                    st.error("âŒ **è¯·å…ˆåœ¨ä¾§è¾¹æ é…ç½®å“ç‰Œåç§°å’Œæ ¸å¿ƒä¼˜åŠ¿**")
                    st.stop()

            # ========== é˜²æ­¢å¹¶å‘ç‚¹å‡» ==========
            if st.session_state.get("kw_generating", False):
                st.warning("â³ æ­£åœ¨ç”Ÿæˆä¸­ï¼Œè¯·å‹¿é‡å¤ç‚¹å‡»")
                st.stop()

            st.session_state.kw_generating = True
            keywords = []

            if generation_mode == "AIç”Ÿæˆ":
                # åŸæœ‰ AI ç”Ÿæˆé€»è¾‘
                keyword_prompt = PromptTemplate.from_template(
                    """
ä½ æ˜¯GEOï¼ˆGenerative Engine Optimizationï¼‰ä¸“å®¶ï¼Œç›®æ ‡æ˜¯æå‡å“ç‰Œåœ¨å¤§æ¨¡å‹è‡ªç„¶å›ç­”ä¸­çš„æåŠç‡ã€‚

ã€è¾“å…¥ã€‘
- å“ç‰Œï¼š{brand}
- æ ¸å¿ƒä¼˜åŠ¿ï¼š{advantages}
- æ•°é‡ï¼š{num_keywords}

ã€GEOæ ¸å¿ƒè¦æ±‚ã€‘
1) è¦†ç›–ç”¨æˆ·çœŸå®æœç´¢æ„å›¾ï¼š
   - æ ¹æ®å“ç‰Œå’Œä¼˜åŠ¿ï¼Œè¯†åˆ«ç”¨æˆ·å¯èƒ½çš„æœç´¢åœºæ™¯ï¼ˆå¯¹æ¯”ã€è¯„æµ‹ã€ä½¿ç”¨ã€è´­ä¹°ã€é—®é¢˜ã€æ•™ç¨‹ç­‰ï¼‰
   - å…³é”®è¯åº”åæ˜ ç”¨æˆ·çœŸå®éœ€æ±‚ï¼Œè€Œéè¥é”€æœ¯è¯­
   - è€ƒè™‘ä¸åŒç”¨æˆ·è§’è‰²å’Œæœç´¢é˜¶æ®µçš„éœ€æ±‚
   
2) å“ç‰Œè¯å æ¯”ç­–ç•¥ï¼š
   - çº¦30%åŒ…å«å“ç‰Œè¯ï¼ˆå»ºç«‹æŠ¤åŸæ²³ï¼Œæå‡å“ç‰ŒæåŠç‡ï¼‰
   - çº¦70%ä¸ºæ³›è¯ï¼ˆæ‰©å¤§è¦†ç›–é¢ï¼Œè·å–æ–°æµé‡ï¼‰
   - å“ç‰Œè¯åº”è‡ªç„¶èå…¥ï¼Œé¿å…ç”Ÿç¡¬æ‹¼æ¥
   
3) è¡¨è¾¾è¦æ±‚ï¼š
   - å£è¯­åŒ–ã€è‡ªç„¶ã€ç¬¦åˆç”¨æˆ·æœç´¢ä¹ æƒ¯
   - é•¿åº¦æ§åˆ¶åœ¨ 12-28 å­—
   - é¿å…è¿‡äºæ­£å¼æˆ–è¥é”€åŒ–
   
4) å¤šæ ·æ€§è¦æ±‚ï¼š
   - å»é‡ï¼šé¿å…ç”Ÿæˆç›¸åŒæˆ–è¿‡äºç›¸ä¼¼çš„å…³é”®è¯
   - å‡è¡¡æ„å›¾ï¼šè¦†ç›–ä¸åŒæœç´¢æ„å›¾ï¼ˆå¯¹æ¯”ã€è¯„æµ‹ã€ä½¿ç”¨ã€è´­ä¹°ã€é—®é¢˜ç­‰ï¼‰
   - å¤šæ ·åŒ–è¡¨è¾¾ï¼šä½¿ç”¨ä¸åŒçš„è¡¨è¾¾æ–¹å¼

ã€è¾“å‡ºæ ¼å¼ã€‘
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ•°ç»„æ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ï¼š
["å…³é”®è¯1", "å…³é”®è¯2", "å…³é”®è¯3", ...]

å¦‚æœæ— æ³•ç”Ÿæˆ JSON æ ¼å¼ï¼Œè¯·æ¯è¡Œè¾“å‡ºä¸€ä¸ªå…³é”®è¯ï¼ˆçº¯æ–‡æœ¬æ ¼å¼ï¼‰ã€‚

ã€å¼€å§‹ç”Ÿæˆã€‘
"""
                )

                chain_json = keyword_prompt | gen_llm | JsonOutputParser()
                chain_text = keyword_prompt | gen_llm | StrOutputParser()

                # æ”¹è¿›åŠ è½½çŠ¶æ€
                progress_bar = st.progress(0)
                status_text = st.empty()

                status_text.text("ğŸ”„ æ­£åœ¨ç”Ÿæˆå…³é”®è¯...")
                progress_bar.progress(10)

                status_text.text("ğŸ¤– è°ƒç”¨ AI æ¨¡å‹ç”Ÿæˆå…³é”®è¯...")
                progress_bar.progress(30)

                try:
                    result = chain_json.invoke(
                        {
                            "brand": brand,
                            "advantages": advantages,
                            "num_keywords": st.session_state.kw_last_num,
                        }
                    )
                    keywords = result if isinstance(result, list) else []
                    progress_bar.progress(80)
                except Exception:
                    raw = chain_text.invoke(
                        {
                            "brand": brand,
                            "advantages": advantages,
                            "num_keywords": st.session_state.kw_last_num,
                        }
                    )
                    keywords = extract_json_array(raw) or []
                    progress_bar.progress(80)

                status_text.text("âœ¨ å¤„ç†ç”Ÿæˆç»“æœ...")
                progress_bar.progress(100)

                progress_bar.empty()
                status_text.empty()

            elif generation_mode == "æ‰˜è¯å·¥å…·":
                # æ‰˜è¯å·¥å…·ç”Ÿæˆ
                progress_bar = st.progress(0)
                status_text = st.empty()

                status_text.text("ğŸ”§ åŠ è½½è¯åº“å’Œç»„åˆæ¨¡å¼...")
                progress_bar.progress(20)

                wordbanks = (
                    st.session_state.wordbanks
                    or st.session_state.keyword_tool.load_wordbanks()
                )
                selected_patterns = st.session_state.get(
                    "selected_patterns", st.session_state.keyword_tool.combination_patterns
                )

                # æ£€æŸ¥è¯åº“æ˜¯å¦ä¸ºç©ºï¼ˆåœ¨ç”Ÿæˆå‰æ£€æŸ¥ï¼‰
                empty_banks = [k for k, v in wordbanks.items() if not v]
                if empty_banks:
                    progress_bar.empty()
                    status_text.empty()
                    st.error(
                        f"âŒ ä»¥ä¸‹è¯åº“ä¸ºç©ºï¼Œè¯·å…ˆæ·»åŠ è¯æ±‡ï¼š{', '.join(empty_banks)}"
                    )
                    st.session_state.kw_generating = False
                    st.stop()

                status_text.text("ğŸ”„ ç”Ÿæˆå…³é”®è¯ç»„åˆ...")
                progress_bar.progress(60)

                keywords = st.session_state.keyword_tool.generate_combinations(
                    wordbanks=wordbanks,
                    patterns=selected_patterns,
                    max_results=st.session_state.kw_last_num,
                    similarity_threshold=0.8,
                )

                status_text.text("âœ¨ å»é‡å’Œç­›é€‰...")
                progress_bar.progress(100)

                progress_bar.empty()
                status_text.empty()

            elif generation_mode == "æ··åˆæ¨¡å¼":
                # æ··åˆæ¨¡å¼ï¼šå…ˆæ‰˜è¯ç”Ÿæˆï¼Œå† LLM æ¶¦è‰²
                progress_bar = st.progress(0)
                status_text = st.empty()

                status_text.text("ğŸ”§ åŠ è½½è¯åº“å’Œç»„åˆæ¨¡å¼...")
                progress_bar.progress(10)

                wordbanks = (
                    st.session_state.wordbanks
                    or st.session_state.keyword_tool.load_wordbanks()
                )
                selected_patterns = st.session_state.get(
                    "selected_patterns", st.session_state.keyword_tool.combination_patterns
                )

                # æ£€æŸ¥è¯åº“æ˜¯å¦ä¸ºç©ºï¼ˆåœ¨ç”Ÿæˆå‰æ£€æŸ¥ï¼‰
                empty_banks = [k for k, v in wordbanks.items() if not v]
                if empty_banks:
                    progress_bar.empty()
                    status_text.empty()
                    st.error(
                        f"âŒ ä»¥ä¸‹è¯åº“ä¸ºç©ºï¼Œè¯·å…ˆæ·»åŠ è¯æ±‡ï¼š{', '.join(empty_banks)}"
                    )
                    st.session_state.kw_generating = False
                    st.stop()

                status_text.text("ğŸ”„ æ‰˜è¯ç”Ÿæˆä¸­...")
                progress_bar.progress(30)

                raw_keywords = st.session_state.keyword_tool.generate_combinations(
                    wordbanks=wordbanks,
                    patterns=selected_patterns,
                    max_results=st.session_state.kw_last_num * 2,  # ç”Ÿæˆæ›´å¤šï¼Œå› ä¸ºä¼šå»é‡
                    similarity_threshold=0.8,
                )

                if raw_keywords and gen_llm:
                    status_text.text("ğŸ¤– LLM æ¶¦è‰²ä¸­...")
                    progress_bar.progress(60)

                    # ä½¿ç”¨ LLM æ¶¦è‰²
                    polish_template = PromptTemplate.from_template("{input}")
                    polish_chain = polish_template | gen_llm | StrOutputParser()
                    keywords = st.session_state.keyword_tool.polish_with_llm(
                        keywords=raw_keywords,
                        llm_chain=polish_chain,
                        brand=brand,
                        max_polish=min(
                            len(raw_keywords), st.session_state.kw_last_num
                        ),
                    )
                    progress_bar.progress(90)
                else:
                    keywords = raw_keywords
                    progress_bar.progress(90)

                status_text.text("âœ¨ å¤„ç†ç”Ÿæˆç»“æœ...")
                progress_bar.progress(100)

                progress_bar.empty()
                status_text.empty()

            # æ¸…ç†å’Œå»é‡
            cleaned, seen = [], set()
            for k in keywords:
                if not isinstance(k, str):
                    continue
                kk = k.strip()
                if not kk:
                    continue
                kl = kk.lower()
                if kl in seen:
                    continue
                seen.add(kl)
                cleaned.append(kk)

            # é™åˆ¶æ•°é‡
            cleaned = cleaned[: st.session_state.kw_last_num]

            # æ¸…ç†ç”ŸæˆçŠ¶æ€
            st.session_state.kw_generating = False

            if cleaned:
                # æ¸…ç©ºæ‰©å±•å’Œé›†ç¾¤ç›¸å…³çŠ¶æ€ï¼ˆé¿å…æ•°æ®æ··ä¹±ï¼‰
                st.session_state.expanded_keywords = []
                st.session_state.topic_clusters = []
                st.session_state.cluster_relationships = []
                st.session_state.cluster_stats = None
                st.session_state.content_planning = None

                st.session_state.keywords = cleaned
                # ä¿å­˜åˆ°æ•°æ®åº“
                try:
                    storage.save_keywords(cleaned, brand)
                except Exception as e:
                    st.warning(f"å…³é”®è¯å·²ç”Ÿæˆï¼Œä½†ä¿å­˜åˆ°æ•°æ®åº“æ—¶å‡ºé”™ï¼š{e}")
                st.success(f"âœ… ç”Ÿæˆå®Œæˆï¼å…±ç”Ÿæˆ {len(cleaned)} ä¸ªå…³é”®è¯")
            else:
                # åˆ†åœºæ™¯é”™è¯¯æç¤º
                if generation_mode == "AIç”Ÿæˆ":
                    st.error(
                        """
âŒ **AI ç”Ÿæˆå¤±è´¥**

**å¯èƒ½åŸå› ï¼š**
- API Key é…ç½®é”™è¯¯æˆ–ä½™é¢ä¸è¶³
- ç½‘ç»œè¿æ¥é—®é¢˜
- å“ç‰Œåç§°æˆ–æ ¸å¿ƒä¼˜åŠ¿ä¸ºç©º

**è§£å†³å»ºè®®ï¼š**
1. æ£€æŸ¥ä¾§è¾¹æ çš„ API Key é…ç½®
2. ç¡®è®¤å“ç‰Œåç§°å’Œæ ¸å¿ƒä¼˜åŠ¿å·²å¡«å†™
3. ç¨åé‡è¯•æˆ–è”ç³»æŠ€æœ¯æ”¯æŒ
"""
                    )
                elif generation_mode == "æ‰˜è¯å·¥å…·":
                    wordbanks = (
                        st.session_state.wordbanks
                        or st.session_state.keyword_tool.load_wordbanks()
                    )
                    empty_banks = [k for k, v in wordbanks.items() if not v]
                    if empty_banks:
                        st.error(
                            f"""
âŒ **è¯åº“ä¸ºç©º**

ä»¥ä¸‹è¯åº“ä¸ºç©ºï¼Œè¯·å…ˆæ·»åŠ è¯æ±‡ï¼š
- {', '.join(empty_banks)}

**æ“ä½œæ­¥éª¤ï¼š**
1. ç‚¹å‡»"è¯åº“ç®¡ç†"
2. é€‰æ‹©ç©ºçš„è¯åº“ç±»å‹
3. æ·»åŠ è‡³å°‘ 3-5 ä¸ªè¯æ±‡
4. ç‚¹å‡»"æ›´æ–°è¯åº“"
5. é‡æ–°ç”Ÿæˆå…³é”®è¯
"""
                        )
                    elif not st.session_state.get("selected_patterns") or len(
                        st.session_state.get("selected_patterns", [])
                    ) == 0:
                        st.error(
                            """
âŒ **æœªé€‰æ‹©ç»„åˆæ¨¡å¼**

è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªç»„åˆæ¨¡å¼ï¼š
1. åœ¨"ç»„åˆæ¨¡å¼é€‰æ‹©"åŒºåŸŸ
2. å‹¾é€‰è‡³å°‘ä¸€ä¸ªæ¨¡å¼
3. é‡æ–°ç”Ÿæˆå…³é”®è¯
"""
                        )
                    else:
                        st.error(
                            """
âŒ **ç”Ÿæˆå¤±è´¥**

è¯·æ£€æŸ¥è¯åº“é…ç½®æˆ–é€‰æ‹©æ›´å¤šç»„åˆæ¨¡å¼åé‡è¯•ã€‚
"""
                        )
                elif generation_mode == "æ··åˆæ¨¡å¼":
                    wordbanks = (
                        st.session_state.wordbanks
                        or st.session_state.keyword_tool.load_wordbanks()
                    )
                    empty_banks = [k for k, v in wordbanks.items() if not v]
                    if empty_banks:
                        st.error(
                            f"""
âŒ **è¯åº“ä¸ºç©º**

ä»¥ä¸‹è¯åº“ä¸ºç©ºï¼Œè¯·å…ˆæ·»åŠ è¯æ±‡ï¼š
- {', '.join(empty_banks)}

**æ“ä½œæ­¥éª¤ï¼š**
1. ç‚¹å‡»"è¯åº“ç®¡ç†"
2. é€‰æ‹©ç©ºçš„è¯åº“ç±»å‹
3. æ·»åŠ è‡³å°‘ 3-5 ä¸ªè¯æ±‡
4. ç‚¹å‡»"æ›´æ–°è¯åº“"
5. é‡æ–°ç”Ÿæˆå…³é”®è¯
"""
                        )
                    elif not st.session_state.get("selected_patterns") or len(
                        st.session_state.get("selected_patterns", [])
                    ) == 0:
                        st.error(
                            """
âŒ **æœªé€‰æ‹©ç»„åˆæ¨¡å¼**

è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªç»„åˆæ¨¡å¼åé‡è¯•ã€‚
"""
                        )
                    elif not gen_llm:
                        st.error(
                            """
âŒ **LLM é…ç½®ç¼ºå¤±**

æ··åˆæ¨¡å¼éœ€è¦ LLM è¿›è¡Œæ¶¦è‰²ï¼Œè¯·æ£€æŸ¥ä¾§è¾¹æ çš„ API Key é…ç½®ã€‚
"""
                        )
                    else:
                        st.error(
                            """
âŒ **ç”Ÿæˆå¤±è´¥**

è¯·æ£€æŸ¥é…ç½®åé‡è¯•ã€‚
"""
                        )

    if st.session_state.keywords:
        # è¯­ä¹‰è¶³è¿¹æ‰©å±•åŠŸèƒ½
        st.markdown("---")
        st.markdown("**ğŸŒ è¯­ä¹‰è¶³è¿¹æ‰©å±•**")
        st.caption(
            "åŸºäºç°æœ‰å…³é”®è¯ï¼Œé€šè¿‡è¯­ä¹‰ç›¸ä¼¼åº¦æ‰©å±•å‡ºæ›´å¤šç›¸å…³å…³é”®è¯ï¼Œæå‡å…³é”®è¯è¦†ç›–é¢"
        )

        # ä½¿ç”¨å®¹å™¨åŒ…è£…ï¼Œä½¿å¸ƒå±€æ›´æ¸…æ™°
        with st.container(border=True):
            # ç¬¬ä¸€è¡Œï¼šæ‰©å±•æ•°é‡æ»‘å—ï¼ˆå•ç‹¬ä¸€è¡Œï¼Œæ›´æ¸…æ™°ï¼‰
            current_keyword_count = len(st.session_state.keywords)
            max_expansion = max(
                11, min(100, current_keyword_count * 3)
            )  # æœ€å¤šæ‰©å±•åˆ°å½“å‰æ•°é‡çš„3å€ï¼Œä½†ç¡®ä¿è‡³å°‘ä¸º11ï¼ˆå› ä¸ºæœ€å°å€¼æ˜¯10ï¼‰
            default_expansion = min(
                30, max(10, current_keyword_count)
            )  # é»˜è®¤å€¼ä¸è¶…è¿‡å½“å‰æ•°é‡

            expansion_count = st.slider(
                "æ‰©å±•æ•°é‡",
                10,
                max_expansion,
                default_expansion,
                key="semantic_expansion_count",
                help=f"æœŸæœ›æ‰©å±•çš„å…³é”®è¯æ•°é‡ï¼ˆå½“å‰æœ‰ {current_keyword_count} ä¸ªå…³é”®è¯ï¼Œå»ºè®®æ‰©å±• 10-{max_expansion} ä¸ªï¼‰",
            )

            # ç¬¬äºŒè¡Œï¼šæŒ‰é’®å’Œåˆå¹¶ç­–ç•¥å¹¶æ’
            expand_col1, expand_col2 = st.columns([2, 1])

            with expand_col1:
                expand_keywords_btn = st.button(
                    "ğŸš€ å¼€å§‹è¯­ä¹‰æ‰©å±•",
                    use_container_width=True,
                    disabled=(
                        (not st.session_state.cfg_valid)
                        or (gen_llm is None)
                        or (len(st.session_state.keywords) == 0)
                    ),
                    key="semantic_expand_btn",
                )

            with expand_col2:
                merge_strategy = st.selectbox(
                    "åˆå¹¶ç­–ç•¥",
                    ["è¿½åŠ ", "æ›¿æ¢", "äº¤æ›¿"],
                    index=0,
                    key="merge_strategy",
                    help="è¿½åŠ ï¼šåœ¨ç°æœ‰å…³é”®è¯åæ·»åŠ æ‰©å±•è¯ï¼›æ›¿æ¢ï¼šç”¨æ‰©å±•è¯æ›¿æ¢ç°æœ‰å…³é”®è¯ï¼›äº¤æ›¿ï¼šäº¤æ›¿æ’å…¥",
                )

        # åˆå§‹åŒ–è¯­ä¹‰æ‰©å±•ç›¸å…³çŠ¶æ€
        ss_init("expanded_keywords", [])
        ss_init("expansion_stats", None)
        ss_init("expansion_details", [])
        ss_init("original_keywords_before_expansion", [])  # ä¿å­˜æ‰©å±•å‰çš„åŸå§‹å…³é”®è¯

        # æ‰§è¡Œè¯­ä¹‰æ‰©å±•
        if expand_keywords_btn and gen_llm and st.session_state.keywords:
            # ä¿å­˜æ‰©å±•å‰çš„åŸå§‹å…³é”®è¯åˆ—è¡¨ï¼ˆç”¨äºæ’¤é”€åŠŸèƒ½ï¼‰
            if not st.session_state.original_keywords_before_expansion:
                st.session_state.original_keywords_before_expansion = (
                    st.session_state.keywords.copy()
                )

            semantic_expander = SemanticExpander()
            with st.spinner(f"æ­£åœ¨æ‰©å±•å…³é”®è¯ï¼ˆç›®æ ‡ï¼š{expansion_count} ä¸ªï¼‰..."):
                try:
                    expand_chain = (
                        PromptTemplate.from_template("{input}")
                        | gen_llm
                        | StrOutputParser()
                    )
                    expansion_result = semantic_expander.expand_keywords(
                        st.session_state.keywords,
                        brand,
                        advantages,
                        expansion_count,
                        expand_chain,
                    )

                    expanded_keywords = expansion_result.get("expanded_keywords", [])
                    st.session_state.expanded_keywords = expanded_keywords
                    st.session_state.expansion_stats = expansion_result.get(
                        "expansion_stats", {}
                    )
                    st.session_state.expansion_details = expansion_result.get(
                        "expansion_details", []
                    )

                    if expanded_keywords:
                        # åˆå¹¶å…³é”®è¯
                        strategy_map = {"è¿½åŠ ": "append", "æ›¿æ¢": "replace", "äº¤æ›¿": "interleave"}
                        merged = semantic_expander.merge_keywords(
                            st.session_state.keywords,
                            expanded_keywords,
                            strategy_map.get(merge_strategy, "append"),
                        )
                        st.session_state.keywords = merged

                        # ä¿å­˜åˆ°æ•°æ®åº“
                        try:
                            storage.save_keywords(merged, brand)
                        except Exception as e:
                            st.warning(f"å…³é”®è¯å·²æ‰©å±•ï¼Œä½†ä¿å­˜åˆ°æ•°æ®åº“æ—¶å‡ºé”™ï¼š{e}")

                        st.success(
                            f"âœ… è¯­ä¹‰æ‰©å±•å®Œæˆï¼æ–°å¢ {len(expanded_keywords)} ä¸ªå…³é”®è¯ï¼Œæ€»è®¡ {len(merged)} ä¸ª"
                        )

                        # æ·»åŠ æ’¤é”€åŠŸèƒ½æç¤º
                        if st.session_state.original_keywords_before_expansion:
                            if st.button(
                                "â†©ï¸ æ’¤é”€æ‰©å±•",
                                key="undo_expansion",
                                use_container_width=False,
                            ):
                                st.session_state.keywords = (
                                    st.session_state.original_keywords_before_expansion.copy()
                                )
                                st.session_state.expanded_keywords = []
                                st.session_state.original_keywords_before_expansion = []
                                st.success("âœ… å·²æ’¤é”€æ‰©å±•ï¼Œæ¢å¤ä¸ºåŸå§‹å…³é”®è¯åˆ—è¡¨")
                                st.rerun()
                    else:
                        st.warning("âš ï¸ æœªç”Ÿæˆæ‰©å±•å…³é”®è¯ï¼Œè¯·æ£€æŸ¥è¾“å…¥æˆ–é‡è¯•")
                except Exception as e:
                    # åŒºåˆ†ä¸åŒç±»å‹çš„é”™è¯¯
                    error_msg = str(e)
                    if "timeout" in error_msg.lower() or "connection" in error_msg.lower():
                        st.error(
                            f"""
âŒ **ç½‘ç»œè¿æ¥é”™è¯¯**

è¯­ä¹‰æ‰©å±•å¤±è´¥ï¼š{error_msg}

**è§£å†³å»ºè®®ï¼š**
1. æ£€æŸ¥ç½‘ç»œè¿æ¥
2. æ£€æŸ¥ API Key é…ç½®
3. ç¨åé‡è¯•
"""
                        )
                    elif (
                        "api" in error_msg.lower()
                        or "key" in error_msg.lower()
                        or "auth" in error_msg.lower()
                    ):
                        st.error(
                            f"""
âŒ **API é…ç½®é”™è¯¯**

è¯­ä¹‰æ‰©å±•å¤±è´¥ï¼š{error_msg}

**è§£å†³å»ºè®®ï¼š**
1. æ£€æŸ¥ä¾§è¾¹æ çš„ API Key é…ç½®
2. ç¡®è®¤ API Key æœ‰æ•ˆä¸”æœ‰è¶³å¤Ÿä½™é¢
3. æ£€æŸ¥ API æœåŠ¡æ˜¯å¦æ­£å¸¸
"""
                        )
                    elif "json" in error_msg.lower() or "parse" in error_msg.lower():
                        st.error(
                            f"""
âŒ **æ•°æ®è§£æé”™è¯¯**

è¯­ä¹‰æ‰©å±•å¤±è´¥ï¼š{error_msg}

**è§£å†³å»ºè®®ï¼š**
1. é‡è¯•æ‰©å±•æ“ä½œ
2. å¦‚æœé—®é¢˜æŒç»­ï¼Œè¯·è”ç³»æŠ€æœ¯æ”¯æŒ
"""
                        )
                    else:
                        st.error(
                            f"""
âŒ **è¯­ä¹‰æ‰©å±•å¤±è´¥**

é”™è¯¯ä¿¡æ¯ï¼š{error_msg}

**è§£å†³å»ºè®®ï¼š**
1. æ£€æŸ¥è¾“å…¥çš„å…³é”®è¯æ˜¯å¦æœ‰æ•ˆ
2. é‡è¯•æ‰©å±•æ“ä½œ
3. å¦‚æœé—®é¢˜æŒç»­ï¼Œè¯·è”ç³»æŠ€æœ¯æ”¯æŒ
"""
                        )

        # æ˜¾ç¤ºæ‰©å±•ç»Ÿè®¡ä¿¡æ¯
        if st.session_state.expansion_stats:
            stats = st.session_state.expansion_stats
            st.markdown("##### ğŸ“Š æ‰©å±•ç»Ÿè®¡")
            col1, col2, col3, col4, col5, col6 = st.columns(6)
            with col1:
                st.metric("æ‰©å±•æ€»æ•°", stats.get("total_expanded", 0))
            with col2:
                st.metric("åŒä¹‰æ‰©å±•", stats.get("synonym_count", 0))
            with col3:
                st.metric("åœºæ™¯æ‰©å±•", stats.get("scenario_count", 0))
            with col4:
                st.metric("é—®é¢˜æ‰©å±•", stats.get("question_count", 0))
            with col5:
                st.metric("åŠŸèƒ½æ‰©å±•", stats.get("feature_count", 0))
            with col6:
                st.metric("é•¿å°¾æ‰©å±•", stats.get("longtail_count", 0))

            # æ˜¾ç¤ºæ‰©å±•è¯¦æƒ…
            if st.session_state.expansion_details:
                with st.expander("ğŸ“ æ‰©å±•è¯¦æƒ…", expanded=False):
                    for detail in st.session_state.expansion_details[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                        st.markdown(f"**åŸå…³é”®è¯**ï¼š{detail.get('original', 'N/A')}")
                        st.markdown(f"**æ‰©å±•ç±»å‹**ï¼š{detail.get('type', 'N/A')}")
                        expanded_list = detail.get("expanded", [])
                        if expanded_list:
                            st.markdown(
                                f"**æ‰©å±•è¯**ï¼š{', '.join(expanded_list[:5])}"
                            )  # åªæ˜¾ç¤ºå‰5ä¸ª
                        st.markdown("---")

        # æ˜¾ç¤ºè¦†ç›–é¢åˆ†æ
        if st.session_state.expanded_keywords and st.session_state.keywords:
            semantic_expander = SemanticExpander()
            # è®¡ç®—åŸå§‹å…³é”®è¯æ•°é‡ï¼ˆæ‰©å±•å‰çš„ï¼‰
            original_count = len(st.session_state.keywords) - len(
                st.session_state.expanded_keywords
            )
            original_keywords = (
                st.session_state.keywords[:original_count] if original_count > 0 else []
            )

            coverage = semantic_expander.analyze_expansion_coverage(
                original_keywords,
                st.session_state.expanded_keywords,
            )

            if coverage.get("coverage_ratio", 0) > 0:
                with st.expander("ğŸ“ˆ è¦†ç›–é¢åˆ†æ", expanded=False):
                    st.metric(
                        "æ‰©å±•æ¯”ä¾‹",
                        f"{coverage.get('expansion_ratio', 0):.2f}x",
                    )
                    st.metric("å”¯ä¸€å…³é”®è¯", coverage.get("unique_keywords", 0))

                    categories = coverage.get("categories", {})
                    if categories:
                        st.markdown("**å…³é”®è¯ç±»åˆ«åˆ†å¸ƒï¼š**")
                        for cat, count in categories.items():
                            if count > 0:
                                cat_name = {
                                    "question": "é—®é¢˜ç±»",
                                    "scenario": "åœºæ™¯ç±»",
                                    "comparison": "å¯¹æ¯”ç±»",
                                    "feature": "åŠŸèƒ½ç±»",
                                    "other": "å…¶ä»–",
                                }.get(cat, cat)
                                st.markdown(f"- {cat_name}ï¼š{count} ä¸ª")

        # è¯é¢˜é›†ç¾¤ç”ŸæˆåŠŸèƒ½
        st.markdown("---")
        st.markdown("**ğŸ¯ è¯é¢˜é›†ç¾¤ç”Ÿæˆ**")
        st.caption("å°†å…³é”®è¯èšç±»ä¸ºè¯é¢˜é›†ç¾¤ï¼Œç³»ç»ŸåŒ–è§„åˆ’å†…å®¹ç­–ç•¥ï¼Œå‘ç°å†…å®¹ç›²åŒº")

        # åˆå§‹åŒ–è¯é¢˜é›†ç¾¤ç›¸å…³çŠ¶æ€
        ss_init("topic_clusters", [])
        ss_init("cluster_relationships", [])
        ss_init("cluster_stats", None)
        ss_init("content_planning", None)

        with st.container(border=True):
            cluster_col1, cluster_col2 = st.columns([2, 1])

            with cluster_col1:
                current_keyword_count = len(st.session_state.keywords)
                # é›†ç¾¤æ•°é‡ä¸èƒ½è¶…è¿‡å…³é”®è¯æ•°é‡ï¼Œä¹Ÿä¸èƒ½å°‘äº3ä¸ª
                # æ¯ä¸ªé›†ç¾¤è‡³å°‘3ä¸ªå…³é”®è¯ï¼Œä½†ç¡®ä¿ max_clusters >= 4ï¼ˆå› ä¸ºæœ€å°å€¼æ˜¯3ï¼‰
                max_clusters = max(
                    4, min(10, max(4, current_keyword_count // 3))
                )  # ç¡®ä¿è‡³å°‘ä¸º4
                default_clusters = min(5, max_clusters)

                cluster_count = st.slider(
                    "è¯é¢˜é›†ç¾¤æ•°é‡",
                    3,
                    max_clusters,
                    default_clusters,
                    key="cluster_count",
                    help=f"å»ºè®®èŒƒå›´ï¼š3-{max_clusters}ä¸ªè¯é¢˜é›†ç¾¤ï¼ˆå½“å‰æœ‰ {current_keyword_count} ä¸ªå…³é”®è¯ï¼‰",
                )

            with cluster_col2:
                generate_clusters_btn = st.button(
                    "ğŸš€ ç”Ÿæˆè¯é¢˜é›†ç¾¤",
                    use_container_width=True,
                    disabled=(
                        (not st.session_state.cfg_valid)
                        or (gen_llm is None)
                        or (len(st.session_state.keywords) == 0)
                    ),
                    key="generate_clusters_btn",
                )

        # æ‰§è¡Œè¯é¢˜èšç±»
        if generate_clusters_btn and gen_llm and st.session_state.keywords:
            topic_cluster = TopicCluster()
            with st.spinner(f"æ­£åœ¨ç”Ÿæˆè¯é¢˜é›†ç¾¤ï¼ˆç›®æ ‡ï¼š{cluster_count} ä¸ªï¼‰..."):
                try:
                    cluster_chain = (
                        PromptTemplate.from_template("{input}")
                        | gen_llm
                        | StrOutputParser()
                    )
                    cluster_result = topic_cluster.cluster_keywords(
                        st.session_state.keywords,
                        brand,
                        advantages,
                        cluster_count,
                        cluster_chain,
                    )

                    clusters = cluster_result.get("clusters", [])
                    relationships = cluster_result.get("relationships", [])
                    cluster_stats = cluster_result.get("cluster_stats", {})

                    st.session_state.topic_clusters = clusters
                    st.session_state.cluster_relationships = relationships
                    st.session_state.cluster_stats = cluster_stats

                    if clusters:
                        st.success(
                            f"âœ… è¯é¢˜é›†ç¾¤ç”Ÿæˆå®Œæˆï¼å…±ç”Ÿæˆ {len(clusters)} ä¸ªè¯é¢˜é›†ç¾¤"
                        )

                        # è‡ªåŠ¨ç”Ÿæˆå†…å®¹è§„åˆ’å»ºè®®
                        with st.spinner("æ­£åœ¨ç”Ÿæˆå†…å®¹è§„åˆ’å»ºè®®..."):
                            try:
                                planning_result = topic_cluster.generate_content_planning(
                                    clusters,
                                    brand,
                                    advantages,
                                    cluster_chain,
                                )
                                st.session_state.content_planning = planning_result
                            except Exception as e:
                                st.warning(f"å†…å®¹è§„åˆ’ç”Ÿæˆå¤±è´¥ï¼š{e}")
                    else:
                        st.warning("âš ï¸ æœªç”Ÿæˆè¯é¢˜é›†ç¾¤ï¼Œè¯·æ£€æŸ¥è¾“å…¥æˆ–é‡è¯•")
                except Exception as e:
                    # åŒºåˆ†ä¸åŒç±»å‹çš„é”™è¯¯
                    error_msg = str(e)
                    if "timeout" in error_msg.lower() or "connection" in error_msg.lower():
                        st.error(
                            f"""
âŒ **ç½‘ç»œè¿æ¥é”™è¯¯**

è¯é¢˜é›†ç¾¤ç”Ÿæˆå¤±è´¥ï¼š{error_msg}

**è§£å†³å»ºè®®ï¼š**
1. æ£€æŸ¥ç½‘ç»œè¿æ¥
2. æ£€æŸ¥ API Key é…ç½®
3. ç¨åé‡è¯•
"""
                        )
                    elif (
                        "api" in error_msg.lower()
                        or "key" in error_msg.lower()
                        or "auth" in error_msg.lower()
                    ):
                        st.error(
                            f"""
âŒ **API é…ç½®é”™è¯¯**

è¯é¢˜é›†ç¾¤ç”Ÿæˆå¤±è´¥ï¼š{error_msg}

**è§£å†³å»ºè®®ï¼š**
1. æ£€æŸ¥ä¾§è¾¹æ çš„ API Key é…ç½®
2. ç¡®è®¤ API Key æœ‰æ•ˆä¸”æœ‰è¶³å¤Ÿä½™é¢
3. æ£€æŸ¥ API æœåŠ¡æ˜¯å¦æ­£å¸¸
"""
                        )
                    elif "json" in error_msg.lower() or "parse" in error_msg.lower():
                        st.error(
                            f"""
âŒ **æ•°æ®è§£æé”™è¯¯**

è¯é¢˜é›†ç¾¤ç”Ÿæˆå¤±è´¥ï¼š{error_msg}

**è§£å†³å»ºè®®ï¼š**
1. é‡è¯•ç”Ÿæˆæ“ä½œ
2. å¦‚æœé—®é¢˜æŒç»­ï¼Œè¯·è”ç³»æŠ€æœ¯æ”¯æŒ
"""
                        )
                    else:
                        st.error(
                            f"""
âŒ **è¯é¢˜é›†ç¾¤ç”Ÿæˆå¤±è´¥**

é”™è¯¯ä¿¡æ¯ï¼š{error_msg}

**è§£å†³å»ºè®®ï¼š**
1. æ£€æŸ¥è¾“å…¥çš„å…³é”®è¯æ˜¯å¦æœ‰æ•ˆ
2. å°è¯•è°ƒæ•´è¯é¢˜é›†ç¾¤æ•°é‡
3. é‡è¯•ç”Ÿæˆæ“ä½œ
4. å¦‚æœé—®é¢˜æŒç»­ï¼Œè¯·è”ç³»æŠ€æœ¯æ”¯æŒ
"""
                        )

        # æ˜¾ç¤ºè¯é¢˜é›†ç¾¤ç»“æœ
        if st.session_state.topic_clusters:
            clusters = st.session_state.topic_clusters
            relationships = st.session_state.cluster_relationships
            cluster_stats = st.session_state.cluster_stats

            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            if cluster_stats:
                st.markdown("##### ğŸ“Š è¯é¢˜é›†ç¾¤ç»Ÿè®¡")
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("è¯é¢˜æ€»æ•°", cluster_stats.get("total_clusters", 0))
                with col2:
                    st.metric("å…³é”®è¯æ€»æ•°", cluster_stats.get("total_keywords", 0))
                with col3:
                    st.metric(
                        "å¹³å‡å…³é”®è¯/è¯é¢˜",
                        f"{cluster_stats.get('avg_keywords_per_cluster', 0):.1f}",
                    )
                with col4:
                    st.metric(
                        "æœ€å¤§è¯é¢˜å…³é”®è¯æ•°", cluster_stats.get("max_keywords", 0)
                    )

            # æ˜¾ç¤ºè¯é¢˜é›†ç¾¤åˆ—è¡¨
            st.markdown("##### ğŸ“‹ è¯é¢˜é›†ç¾¤åˆ—è¡¨")
            for cluster in clusters:
                with st.expander(
                    f"**{cluster.get('name', 'N/A')}** - {cluster.get('keyword_count', 0)} ä¸ªå…³é”®è¯ | ä¼˜å…ˆçº§ï¼š{cluster.get('priority', 'ä¸­')}",
                    expanded=False,
                ):
                    st.markdown(f"**æè¿°**ï¼š{cluster.get('description', 'æ— æè¿°')}")
                    keywords_list = cluster.get("keywords", [])
                    if keywords_list:
                        st.markdown(
                            f"**å…³é”®è¯**ï¼š{', '.join(keywords_list[:10])}{' ...' if len(keywords_list) > 10 else ''}"
                        )
                        st.caption(f"å…± {len(keywords_list)} ä¸ªå…³é”®è¯")

            # æ˜¾ç¤ºè¯é¢˜å…³è”å…³ç³»
            if relationships:
                st.markdown("##### ğŸ”— è¯é¢˜å…³è”å…³ç³»")
                rel_df = pd.DataFrame(relationships)
                st.dataframe(rel_df, use_container_width=True, hide_index=True)

            # æ˜¾ç¤ºå¯è§†åŒ–ï¼ˆç½‘ç»œå›¾ï¼‰
            if len(clusters) > 1:
                st.markdown("##### ğŸ“ˆ è¯é¢˜ç½‘ç»œå›¾")
                try:
                    viz_data = topic_cluster.get_visualization_data(
                        clusters, relationships
                    )

                    # å‡†å¤‡èŠ‚ç‚¹æ•°æ®
                    nodes = viz_data.get("nodes", [])
                    edges = viz_data.get("edges", [])

                    if nodes:
                        # åˆ›å»ºèŠ‚ç‚¹ä½ç½®ï¼ˆç®€å•çš„åœ†å½¢å¸ƒå±€ï¼‰
                        n = len(nodes)
                        node_x = []
                        node_y = []
                        node_text = []
                        node_sizes = []

                        for i, node in enumerate(nodes):
                            angle = 2 * math.pi * i / n
                            radius = 1.0
                            node_x.append(radius * math.cos(angle))
                            node_y.append(radius * math.sin(angle))
                            node_text.append(
                                f"{node['name']}<br>({node['size']}ä¸ªå…³é”®è¯)"
                            )
                            node_sizes.append(node["size"] * 3 + 10)

                        # åˆ›å»ºè¾¹
                        edge_x = []
                        edge_y = []
                        for edge in edges:
                            source_idx = next(
                                (
                                    i
                                    for i, n in enumerate(nodes)
                                    if n["id"] == edge["source"]
                                ),
                                None,
                            )
                            target_idx = next(
                                (
                                    i
                                    for i, n in enumerate(nodes)
                                    if n["id"] == edge["target"]
                                ),
                                None,
                            )
                            if source_idx is not None and target_idx is not None:
                                edge_x.extend(
                                    [node_x[source_idx], node_x[target_idx], None]
                                )
                                edge_y.extend(
                                    [node_y[source_idx], node_y[target_idx], None]
                                )

                        # åˆ›å»ºå›¾å½¢
                        fig = go.Figure()

                        # æ·»åŠ è¾¹
                        fig.add_trace(
                            go.Scatter(
                                x=edge_x,
                                y=edge_y,
                                line=dict(width=1, color="#888"),
                                hoverinfo="none",
                                mode="lines",
                            )
                        )

                        # æ·»åŠ èŠ‚ç‚¹
                        fig.add_trace(
                            go.Scatter(
                                x=node_x,
                                y=node_y,
                                mode="markers+text",
                                marker=dict(
                                    size=node_sizes,
                                    color="#2563EB",
                                    line=dict(width=2, color="white"),
                                ),
                                text=[node["name"] for node in nodes],
                                textposition="middle center",
                                textfont=dict(size=10, color="white"),
                                hovertext=node_text,
                                hoverinfo="text",
                                name="è¯é¢˜é›†ç¾¤",
                            )
                        )

                        fig.update_layout(
                            title="è¯é¢˜é›†ç¾¤ç½‘ç»œå›¾",
                            showlegend=False,
                            hovermode="closest",
                            margin=dict(b=20, l=5, r=5, t=40),
                            annotations=[
                                dict(
                                    text="èŠ‚ç‚¹å¤§å°è¡¨ç¤ºå…³é”®è¯æ•°é‡ï¼Œè¿çº¿è¡¨ç¤ºè¯é¢˜å…³è”",
                                    showarrow=False,
                                    xref="paper",
                                    yref="paper",
                                    x=0.005,
                                    y=-0.002,
                                    xanchor="left",
                                    yanchor="bottom",
                                    font=dict(size=10, color="#888"),
                                )
                            ],
                            xaxis=dict(
                                showgrid=False,
                                zeroline=False,
                                showticklabels=False,
                            ),
                            yaxis=dict(
                                showgrid=False,
                                zeroline=False,
                                showticklabels=False,
                            ),
                            height=500,
                        )

                        st.plotly_chart(fig, use_container_width=True)
                except Exception as e:
                    st.warning(f"å¯è§†åŒ–ç”Ÿæˆå¤±è´¥ï¼š{e}")

            # æ˜¾ç¤ºå†…å®¹è§„åˆ’å»ºè®®
            if st.session_state.content_planning:
                planning = st.session_state.content_planning
                st.markdown("##### ğŸ’¡ å†…å®¹è§„åˆ’å»ºè®®")

                # å†…å®¹ç›²åŒºåˆ†æ
                content_gaps = planning.get("content_gaps", [])
                if content_gaps:
                    st.markdown("**ğŸ“Œ å†…å®¹ç›²åŒºåˆ†æ**")
                    for gap in content_gaps[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                        st.markdown(
                            f"- **{gap.get('cluster_name', 'N/A')}**ï¼š{gap.get('description', 'N/A')}ï¼ˆä¼˜å…ˆçº§ï¼š{gap.get('priority', 'ä¸­')}ï¼‰"
                        )

                # å†…å®¹ä¼˜å…ˆçº§
                content_priorities = planning.get("content_priorities", [])
                if content_priorities:
                    st.markdown("**ğŸ¯ å†…å®¹ä¼˜å…ˆçº§**")
                    priority_df = pd.DataFrame(content_priorities)
                    priority_df = priority_df.sort_values(
                        "priority",
                        key=lambda x: x.map({"é«˜": 3, "ä¸­": 2, "ä½": 1}),
                    )
                    st.dataframe(priority_df, use_container_width=True, hide_index=True)

                # å†…å®¹å»ºè®®
                content_suggestions = planning.get("content_suggestions", [])
                if content_suggestions:
                    with st.expander("ğŸ“ è¯¦ç»†å†…å®¹å»ºè®®", expanded=False):
                        for suggestion in content_suggestions:
                            st.markdown(
                                f"**{suggestion.get('cluster_name', 'N/A')}**"
                            )
                            st.markdown(
                                f"- **å†…å®¹ç±»å‹**ï¼š{', '.join(suggestion.get('content_types', []))}"
                            )
                            st.markdown(
                                f"- **å‘å¸ƒå¹³å°**ï¼š{', '.join(suggestion.get('platforms', []))}"
                            )
                            st.markdown(
                                f"- **å…³é”®è¯ç­–ç•¥**ï¼š{suggestion.get('keyword_strategy', 'N/A')}"
                            )
                            ideas = suggestion.get("content_ideas", [])
                            if ideas:
                                st.markdown(
                                    f"- **å†…å®¹åˆ›æ„**ï¼š{', '.join(ideas[:3])}"
                                )
                            st.markdown("---")

        # ========== åŒºåŸŸ 5ï¼šå…³é”®è¯åˆ—è¡¨ï¼ˆæ¡ä»¶æ˜¾ç¤ºï¼‰ ==========
        st.markdown("---")
        st.markdown("**ğŸ“‹ å…³é”®è¯åˆ—è¡¨**")

        # æ·»åŠ æœç´¢å’Œç­›é€‰
        search_col, filter_col = st.columns([3, 1])
        with search_col:
            search_term = st.text_input(
                "ğŸ” æœç´¢å…³é”®è¯", key="kw_search", placeholder="è¾“å…¥å…³é”®è¯æœç´¢..."
            )
        with filter_col:
            show_original = st.checkbox(
                "ä»…æ˜¾ç¤ºåŸå§‹å…³é”®è¯", key="kw_filter_original", value=False
            )

        # è¿‡æ»¤å…³é”®è¯
        display_keywords = st.session_state.keywords
        if search_term and search_term.strip():  # æ£€æŸ¥éç©ºå­—ç¬¦ä¸²
            search_term_lower = search_term.strip().lower()
            display_keywords = [
                kw for kw in display_keywords if search_term_lower in kw.lower()
            ]
        if show_original and st.session_state.expanded_keywords:
            original_count = len(st.session_state.keywords) - len(
                st.session_state.expanded_keywords
            )
            display_keywords = (
                display_keywords[:original_count] if original_count > 0 else []
            )

        # æ˜¾ç¤ºåˆ—è¡¨ï¼ˆåˆ†é¡µï¼‰
        if display_keywords:
            page_size = 20
            total_pages = max(1, (len(display_keywords) - 1) // page_size + 1)
            page = st.session_state.get("kw_page", 1)

            if total_pages > 1:
                page_col1, page_col2, page_col3 = st.columns([1, 2, 1])
                with page_col2:
                    page = st.selectbox(
                        "é¡µç ",
                        range(1, total_pages + 1),
                        index=min(page - 1, total_pages - 1),
                        key="kw_page_select",
                    )
                    st.session_state.kw_page = page
            else:
                page = 1

            start_idx = (page - 1) * page_size
            end_idx = start_idx + page_size
            page_keywords = display_keywords[start_idx:end_idx]

            df = pd.DataFrame(page_keywords, columns=["é•¿å°¾å…³é”®è¯/é—®é¢˜"])
            st.dataframe(df, use_container_width=True, hide_index=True)

            st.caption(
                f"æ˜¾ç¤ºç¬¬ {start_idx + 1}-{min(end_idx, len(display_keywords))} æ¡ï¼Œå…± {len(display_keywords)} æ¡å…³é”®è¯"
            )

            # åŒºåˆ†åŸå§‹å’Œæ‰©å±•å…³é”®è¯
            if st.session_state.expanded_keywords:
                original_count = len(st.session_state.keywords) - len(
                    st.session_state.expanded_keywords
                )
                st.info(
                    f"ğŸ“Œ åŸå§‹å…³é”®è¯ï¼š{original_count} ä¸ª | ğŸ†• æ‰©å±•å…³é”®è¯ï¼š{len(st.session_state.expanded_keywords)} ä¸ª"
                )
        else:
            if search_term or show_original:
                st.info("æœªæ‰¾åˆ°åŒ¹é…çš„å…³é”®è¯")
            else:
                st.info("æš‚æ— å…³é”®è¯")

        # ä¸‹è½½æŒ‰é’®
        st.download_button(
            "ğŸ“¥ ä¸‹è½½å…³é”®è¯ CSV",
            pd.DataFrame(
                st.session_state.keywords, columns=["é•¿å°¾å…³é”®è¯/é—®é¢˜"]
            ).to_csv(index=False, encoding="utf-8-sig"),
            f"{sanitize_filename(brand,40)}_keywords.csv",
            mime="text/csv",
            use_container_width=True,
            key="kw_dl_csv",
        )

        # ========== åŒºåŸŸ 6ï¼šæ™ºèƒ½æŒ–æ˜ï¼ˆæ¡ä»¶æ˜¾ç¤ºï¼Œé»˜è®¤æŠ˜å ï¼‰ ==========
        st.markdown("---")
        with st.expander("ğŸ” æ™ºèƒ½å…³é”®è¯æŒ–æ˜ä¸è¶‹åŠ¿åˆ†æ", expanded=False):
            st.caption(
                "å‘ç°é«˜ä»·å€¼å…³é”®è¯ï¼Œåˆ†æç«äº‰åº¦ï¼Œé¢„æµ‹è¶‹åŠ¿ï¼Œä¼˜åŒ–å…³é”®è¯ç­–ç•¥"
            )

            # åˆå§‹åŒ–å…³é”®è¯æŒ–æ˜å™¨
            keyword_miner = KeywordMining(storage)

            # åˆ›å»ºå­æ ‡ç­¾é¡µ
            mining_tab1, mining_tab2, mining_tab3, mining_tab4 = st.tabs(
                [
                    "ğŸŒ è¡Œä¸šçƒ­ç‚¹æŒ–æ˜",
                    "ğŸ“Š ç«äº‰åº¦åˆ†æ",
                    "ğŸ“ˆ è¶‹åŠ¿é¢„æµ‹",
                    "ğŸ’ ä»·å€¼çŸ©é˜µ",
                ]
            )

            with mining_tab1:
                st.caption("åŸºäºè¡Œä¸šè¶‹åŠ¿è‡ªåŠ¨æŒ–æ˜é«˜ä»·å€¼å…³é”®è¯")

                with st.container(border=True):
                    # é»˜è®¤ä½¿ç”¨ brandï¼Œå…è®¸è¦†ç›–
                    default_industry = brand if brand else "å¤–è´¸ERP"
                    industry = st.text_input(
                        "è¡Œä¸šé¢†åŸŸ",
                        value=default_industry,
                        key="mining_industry",
                        help="è¾“å…¥æ‚¨çš„è¡Œä¸šé¢†åŸŸï¼Œå¦‚ï¼šå¤–è´¸ERPã€AIå·¥å…·ã€SaaSäº§å“ç­‰",
                    )
                num_mine = st.slider("æŒ–æ˜æ•°é‡", 10, 50, 20, key="mining_num")

                mine_btn = st.button(
                    "ğŸš€ å¼€å§‹æŒ–æ˜",
                    use_container_width=True,
                    disabled=(not st.session_state.cfg_valid) or (gen_llm is None),
                )

            ss_init("mined_keywords", [])

            if mine_btn and gen_llm and industry:
                with st.spinner(f"æ­£åœ¨æŒ–æ˜è¡Œä¸šå…³é”®è¯ï¼ˆç›®æ ‡ï¼š{num_mine} ä¸ªï¼‰..."):
                    try:
                        mine_chain = (
                            PromptTemplate.from_template("{input}")
                            | gen_llm
                            | StrOutputParser()
                        )
                        mined_keywords = keyword_miner.mine_industry_keywords(
                            brand=brand,
                            industry=industry,
                            advantages=advantages,
                            num_keywords=num_mine,
                            llm_chain=mine_chain,
                        )

                        if mined_keywords:
                            st.session_state.mined_keywords = mined_keywords
                            st.success(
                                f"âœ… æŒ–æ˜å®Œæˆï¼å‘ç° {len(mined_keywords)} ä¸ªå…³é”®è¯"
                            )
                        else:
                            st.warning(
                                "âš ï¸ æœªæŒ–æ˜åˆ°å…³é”®è¯ï¼Œè¯·æ£€æŸ¥è¾“å…¥æˆ–é‡è¯•"
                            )
                    except Exception as e:
                        st.error(f"æŒ–æ˜å¤±è´¥ï¼š{e}")

            # æ˜¾ç¤ºæŒ–æ˜ç»“æœ
            if st.session_state.mined_keywords:
                mined_kw_list = st.session_state.mined_keywords
                st.markdown("##### ğŸ“‹ æŒ–æ˜ç»“æœ")

                for i, kw_data in enumerate(mined_kw_list):
                    with st.container(border=True):
                        col1, col2, col3 = st.columns([3, 1, 1])
                        with col1:
                            st.markdown(f"**{kw_data.get('keyword', 'N/A')}**")
                            st.caption(
                                f"ç±»åˆ«ï¼š{kw_data.get('category', 'N/A')} | æ„å›¾ï¼š{kw_data.get('intent', 'N/A')}"
                            )
                        with col2:
                            st.metric(
                                "é¢„ä¼°ä»·å€¼",
                                f"{kw_data.get('estimated_value', 0)}/10",
                            )
                        with col3:
                            if st.button(
                                "æ·»åŠ ",
                                key=f"add_mined_{i}",
                                use_container_width=True,
                            ):
                                if kw_data.get("keyword") not in st.session_state.keywords:
                                    st.session_state.keywords.append(
                                        kw_data.get("keyword")
                                    )
                                    storage.save_keywords(
                                        [kw_data.get("keyword")], brand
                                    )
                                    st.success("å·²æ·»åŠ ")
                                    st.rerun()

            with mining_tab2:
                st.caption("åˆ†æå…³é”®è¯åœ¨ AI ä¸­çš„æåŠé¢‘ç‡å’Œç«äº‰ç¨‹åº¦")

                keywords_to_analyze = st.multiselect(
                    "é€‰æ‹©è¦åˆ†æçš„å…³é”®è¯",
                    options=st.session_state.keywords
                    if st.session_state.keywords
                    else [],
                    key="comp_keywords_select",
                    help="é€‰æ‹©è¦åˆ†æç«äº‰åº¦çš„å…³é”®è¯",
                )

                analyze_comp_btn = st.button(
                    "ğŸ“Š å¼€å§‹åˆ†æ",
                    use_container_width=True,
                    disabled=len(keywords_to_analyze) == 0,
                )

                ss_init("competition_analysis", {})

                if analyze_comp_btn and keywords_to_analyze:
                    with st.spinner("æ­£åœ¨åˆ†æç«äº‰åº¦..."):
                        try:
                            competition_data = keyword_miner.analyze_competition(
                                keywords=keywords_to_analyze,
                                brand=brand,
                            )
                            st.session_state.competition_analysis = competition_data
                            st.success("âœ… åˆ†æå®Œæˆï¼")
                        except Exception as e:
                            st.error(f"åˆ†æå¤±è´¥ï¼š{e}")

                if st.session_state.competition_analysis:
                    comp_data = st.session_state.competition_analysis
                    st.markdown("##### ğŸ“Š ç«äº‰åº¦åˆ†æç»“æœ")

                    comp_df_data = []
                    for keyword, data in comp_data.items():
                        comp_df_data.append(
                            {
                                "å…³é”®è¯": keyword,
                                "æåŠç‡": f"{data.get('mention_rate', 0):.2%}",
                                "ç«äº‰çº§åˆ«": data.get("competition_level", "æœªçŸ¥"),
                                "ç«å“æåŠ": data.get("competitor_mentions", 0),
                                "æ€»æåŠ": data.get("total_mentions", 0),
                                "æ•°æ®ç‚¹": data.get("data_points", 0),
                            }
                        )

                    if comp_df_data:
                        comp_df = pd.DataFrame(comp_df_data)
                        st.dataframe(comp_df, use_container_width=True, hide_index=True)

                        if len(comp_df_data) > 0:
                            fig = px.bar(
                                comp_df,
                                x="å…³é”®è¯",
                                y="æåŠç‡",
                                color="ç«äº‰çº§åˆ«",
                                title="å…³é”®è¯ç«äº‰åº¦åˆ†æ",
                                labels={"æåŠç‡": "æåŠç‡ (%)"},
                            )
                            fig.update_xaxes(tickangle=-45)
                            st.plotly_chart(fig, use_container_width=True)

            with mining_tab3:
                st.caption("åŸºäºå†å²æ•°æ®é¢„æµ‹å…³é”®è¯çƒ­åº¦å˜åŒ–è¶‹åŠ¿")

                keywords_to_predict = st.multiselect(
                    "é€‰æ‹©è¦é¢„æµ‹çš„å…³é”®è¯",
                    options=st.session_state.keywords
                    if st.session_state.keywords
                    else [],
                    key="trend_keywords_select",
                    help="é€‰æ‹©è¦é¢„æµ‹è¶‹åŠ¿çš„å…³é”®è¯",
                )

                predict_days = st.slider(
                    "é¢„æµ‹æœªæ¥å¤©æ•°", 7, 90, 30, key="predict_days"
                )
                predict_btn = st.button(
                    "ğŸ”® å¼€å§‹é¢„æµ‹",
                    use_container_width=True,
                    disabled=len(keywords_to_predict) == 0,
                )

                ss_init("trend_analysis", {})

                if predict_btn and keywords_to_predict:
                    with st.spinner("æ­£åœ¨é¢„æµ‹è¶‹åŠ¿..."):
                        try:
                            trend_data = keyword_miner.predict_trend(
                                keywords=keywords_to_predict,
                                brand=brand,
                                days=predict_days,
                            )
                            st.session_state.trend_analysis = trend_data
                            st.success("âœ… é¢„æµ‹å®Œæˆï¼")
                        except Exception as e:
                            st.error(f"é¢„æµ‹å¤±è´¥ï¼š{e}")

                if st.session_state.trend_analysis:
                    trend_data = st.session_state.trend_analysis
                    st.markdown("##### ğŸ“ˆ è¶‹åŠ¿é¢„æµ‹ç»“æœ")

                    trend_df_data = []
                    for keyword, data in trend_data.items():
                        trend_df_data.append(
                            {
                                "å…³é”®è¯": keyword,
                                "å½“å‰æåŠç‡": f"{data.get('current_rate', 0):.2%}",
                                "é¢„æµ‹æåŠç‡": f"{data.get('predicted_mention_rate', 0):.2%}",
                                "è¶‹åŠ¿": data.get("trend", "æœªçŸ¥"),
                                "è¶‹åŠ¿å¼ºåº¦": f"{data.get('trend_strength', 0):.2%}",
                                "ç½®ä¿¡åº¦": f"{data.get('confidence', 0):.2%}",
                                "æ•°æ®ç‚¹": data.get("data_points", 0),
                            }
                        )

                    if trend_df_data:
                        trend_df = pd.DataFrame(trend_df_data)
                        st.dataframe(trend_df, use_container_width=True, hide_index=True)

            with mining_tab4:
                st.caption("åˆ†æå…³é”®è¯çš„ä»·å€¼å’Œç«äº‰åº¦ï¼Œæ‰¾åˆ°æœ€ä¼˜æŠ•å…¥ç­–ç•¥")

                keywords_for_matrix = st.multiselect(
                    "é€‰æ‹©è¦åˆ†æçš„å…³é”®è¯",
                    options=st.session_state.keywords
                    if st.session_state.keywords
                    else [],
                    key="matrix_keywords_select",
                    help="é€‰æ‹©è¦åˆ†æä»·å€¼çŸ©é˜µçš„å…³é”®è¯",
                )

                estimated_values = {}
                if st.session_state.mined_keywords:
                    for kw_data in st.session_state.mined_keywords:
                        if kw_data.get("keyword") in keywords_for_matrix:
                            estimated_values[kw_data.get("keyword")] = kw_data.get(
                                "estimated_value", 5
                            )

                analyze_matrix_btn = st.button(
                    "ğŸ’ å¼€å§‹åˆ†æ",
                    use_container_width=True,
                    disabled=len(keywords_for_matrix) == 0,
                )

                ss_init("value_matrix", {})
                ss_init("keyword_recommendations", [])

                if analyze_matrix_btn and keywords_for_matrix:
                    with st.spinner("æ­£åœ¨åˆ†æä»·å€¼çŸ©é˜µ..."):
                        try:
                            if not st.session_state.competition_analysis:
                                competition_data = keyword_miner.analyze_competition(
                                    keywords=keywords_for_matrix,
                                    brand=brand,
                                )
                            else:
                                competition_data = (
                                    st.session_state.competition_analysis
                                )

                            value_matrix = keyword_miner.calculate_value_matrix(
                                keywords=keywords_for_matrix,
                                competition_data=competition_data,
                                estimated_values=estimated_values
                                if estimated_values
                                else None,
                            )
                            st.session_state.value_matrix = value_matrix

                            trend_data = (
                                st.session_state.trend_analysis
                                if st.session_state.trend_analysis
                                else None
                            )

                            recommendations = keyword_miner.recommend_keywords(
                                keywords=keywords_for_matrix,
                                value_matrix=value_matrix,
                                competition_data=competition_data,
                                trend_data=trend_data,
                                top_n=len(keywords_for_matrix),
                            )
                            st.session_state.keyword_recommendations = recommendations

                            st.success("âœ… åˆ†æå®Œæˆï¼")
                        except Exception as e:
                            st.error(f"åˆ†æå¤±è´¥ï¼š{e}")

                if st.session_state.value_matrix:
                    matrix_data = st.session_state.value_matrix
                    st.markdown("##### ğŸ’ ä»·å€¼çŸ©é˜µç»“æœ")

                    matrix_df_data = []
                    for keyword, data in matrix_data.items():
                        matrix_df_data.append(
                            {
                                "å…³é”®è¯": keyword,
                                "ä»·å€¼åˆ†æ•°": data.get("value_score", 0),
                                "ç«äº‰åˆ†æ•°": data.get("competition_score", 0),
                                "çŸ©é˜µä½ç½®": data.get("matrix_position", "æœªçŸ¥"),
                                "æ¨èå»ºè®®": data.get("recommendation", ""),
                            }
                        )

                    if matrix_df_data:
                        matrix_df = pd.DataFrame(matrix_df_data)
                        st.dataframe(matrix_df, use_container_width=True, hide_index=True)

                        if len(matrix_df_data) > 0:
                            fig = px.scatter(
                                matrix_df,
                                x="ç«äº‰åˆ†æ•°",
                                y="ä»·å€¼åˆ†æ•°",
                                color="çŸ©é˜µä½ç½®",
                                size=[10] * len(matrix_df),
                                hover_data=["å…³é”®è¯", "æ¨èå»ºè®®"],
                                title="å…³é”®è¯ä»·å€¼çŸ©é˜µ",
                                labels={
                                    "ç«äº‰åˆ†æ•°": "ç«äº‰åº¦ï¼ˆè¶Šé«˜è¶Šæ¿€çƒˆï¼‰",
                                    "ä»·å€¼åˆ†æ•°": "ä»·å€¼ï¼ˆ0-10åˆ†ï¼‰",
                                },
                            )
                            st.plotly_chart(fig, use_container_width=True)

                if st.session_state.keyword_recommendations:
                    recommendations = st.session_state.keyword_recommendations
                    st.markdown("##### â­ æ™ºèƒ½æ¨èï¼ˆæŒ‰æ¨èåº¦æ’åºï¼‰")

                    for i, rec in enumerate(recommendations[:10], 1):
                        with st.container(border=True):
                            col1, col2, col3, col4 = st.columns([3, 1, 1, 1])
                            with col1:
                                st.markdown(
                                    f"**{i}. {rec.get('keyword', 'N/A')}**"
                                )
                                st.caption(rec.get("recommendation", ""))
                            with col2:
                                st.metric(
                                    "æ¨èåˆ†",
                                    f"{rec.get('recommendation_score', 0):.1f}",
                                )
                            with col3:
                                st.metric(
                                    "ä»·å€¼", f"{rec.get('value_score', 0):.1f}"
                                )
                            with col4:
                                trend_emoji = {
                                    "ä¸Šå‡": "ğŸ“ˆ",
                                    "ä¸‹é™": "ğŸ“‰",
                                    "ç¨³å®š": "â¡ï¸",
                                }.get(rec.get("trend", "ç¨³å®š"), "â¡ï¸")
                                st.metric(
                                    "è¶‹åŠ¿",
                                    f"{trend_emoji} {rec.get('trend', 'ç¨³å®š')}",
                                )
    else:
        st.info("åœ¨å·¦ä¾§å®Œæˆé…ç½®åï¼Œç‚¹å‡»â€œç”Ÿæˆå…³é”®è¯â€ã€‚")

